# A Comparative Analysis of Parameter Optimization and Feature Engineering for Time-Series ML Models

This repository contains the implementation of the research paper titled "A Comparative Analysis of Parameter Optimization and Feature Engineering for Time-Series ML Models" by Prabhat Karlekar, Pranali Nikam, and Puneet Bakshi, conducted at the Centre for Development of Advanced Computing (C-DAC), Pune, India.

**Project Overview**

This project investigates the impact of parameter optimization and feature engineering on time-series machine learning models, specifically Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) models, in a Federated Learning (FL) context. The study focuses on:

        Analyzing the effects of hyperparameters (e.g., window size, batch size, learning rate, number of layers, dropout rate) on model performance.
        
        Evaluating feature engineering strategies such as lag features, moving averages, technical indicators (e.g., RSI, MACD), and scaling methods.
        
        Comparing LSTM and GRU model performance on real-world stock market datasets (e.g., Tata Motors, Reliance) using metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), and RÂ² score.
        
        Providing practical guidelines for robust hyperparameter selection and standardized feature engineering to enhance model accuracy and stability in federated settings.

The experiments were conducted on the PARAM Siddhi-AI supercomputer, a high-performance computing system under the National Supercomputing Mission (NSM), India.
